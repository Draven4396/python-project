{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-a2c67b6213a4>:6: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.10 it will stop working\n",
      "  from collections import Iterable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# -*- coding:utf-8 -*-\n",
    "#处理测试集\n",
    "\n",
    "import codecs\n",
    "import collections\n",
    "from collections import Iterable\n",
    "import re\n",
    "import torch\n",
    "def originHandle():\n",
    "    with open('./renmin.txt', 'r',encoding='utf8') as inp, open('./renmin2.txt', 'w') as outp:\n",
    "        for line in inp.readlines():\n",
    "            line = line.split('  ')\n",
    "            i = 1\n",
    "            while i < len(line) - 1:\n",
    "                if line[i][0] == '[':\n",
    "                    outp.write(line[i].split('/')[0][1:])\n",
    "                    i += 1\n",
    "                    while i < len(line) - 1 and line[i].find(']') == -1:\n",
    "                        if line[i] != '':\n",
    "                            outp.write(line[i].split('/')[0])\n",
    "                        i += 1\n",
    "                    outp.write(line[i].split('/')[0].strip() + '/' + line[i][-2:]+' ')\n",
    "                elif line[i].split('/')[1] == 'nr':\n",
    "                    word = line[i].split('/')[0]\n",
    "                    i += 1\n",
    "                    if i < len(line) - 1 and line[i].split('/')[1] == 'nr':\n",
    "                        outp.write(word + line[i].split('/')[0] + '/nr ')\n",
    "                    else:\n",
    "                        outp.write(word + '/nr ')\n",
    "                        continue\n",
    "                else:\n",
    "                    outp.write(line[i] + ' ')\n",
    "                i += 1\n",
    "            outp.write('\\n')\n",
    "def originHandle2():\n",
    "    with codecs.open('renmin2.txt', 'r', encoding='gbk') as inp, codecs.open('renmin3.txt', 'w', encoding='gbk') as outp:\n",
    "        for line in inp.readlines():\n",
    "            line = line.split(' ')\n",
    "            i = 0\n",
    "            while i<len(line)-1:\n",
    "                if line[i]=='':\n",
    "                    i+=1\n",
    "                    continue\n",
    "                word = line[i].split('/')[0]\n",
    "                tag = line[i].split('/')[1]\n",
    "                if tag=='nr' or tag=='ns' or tag=='nt':\n",
    "                    outp.write(word[0]+\"/B \")\n",
    "                    for j in word[1:len(word)]:\n",
    "                        if j!=' ':\n",
    "                            outp.write(j+\"/I \")\n",
    "                else:\n",
    "                    for wor in word:\n",
    "                        outp.write(wor+'/O ')\n",
    "                i+=1\n",
    "            outp.write('\\n')\n",
    "def sentence2split():\n",
    "    with codecs.open('./renmin3.txt','r','gbk') as inp,codecs.open('./renmin4.txt','w','gbk') as outp:\n",
    "        texts = inp.read()\n",
    "        sentences = re.split('[，。！？、‘’“”:]/[O]', texts)\n",
    "        for sentence in sentences:\n",
    "\t        if sentence != \" \":\n",
    "\t\t        outp.write(sentence.strip()+'\\n')\n",
    "def originHandle3():\n",
    "    with open('./validation.txt', 'r',encoding='utf8') as inp, open('./validation2.txt', 'w') as outp:\n",
    "        for line in inp.readlines():\n",
    "            line = line.split('  ')\n",
    "            i = 1\n",
    "            while i < len(line) - 1:\n",
    "                if line[i][0] == '[':\n",
    "                    outp.write(line[i].split('/')[0][1:])\n",
    "                    i += 1\n",
    "                    while i < len(line) - 1 and line[i].find(']') == -1:\n",
    "                        if line[i] != '':\n",
    "                            outp.write(line[i].split('/')[0])\n",
    "                        i += 1\n",
    "                    outp.write(line[i].split('/')[0].strip() + '/' + line[i][-2:]+' ')\n",
    "                elif line[i].split('/')[1] == 'nr':\n",
    "                    word = line[i].split('/')[0]\n",
    "                    i += 1\n",
    "                    if i < len(line) - 1 and line[i].split('/')[1] == 'nr':\n",
    "                        outp.write(word + line[i].split('/')[0] + '/nr ')\n",
    "                    else:\n",
    "                        outp.write(word + '/nr ')\n",
    "                        continue\n",
    "                else:\n",
    "                    outp.write(line[i] + ' ')\n",
    "                i += 1\n",
    "            outp.write('\\n')\n",
    "def originHandle4():\n",
    "    with codecs.open('validation2.txt', 'r', encoding='gbk') as inp, codecs.open('validation3.txt', 'w', encoding='gbk') as outp:\n",
    "        for line in inp.readlines():\n",
    "            line = line.split(' ')\n",
    "            i = 0\n",
    "            while i<len(line)-1:\n",
    "                if line[i]=='':\n",
    "                    i+=1\n",
    "                    continue\n",
    "                word = line[i].split('/')[0]\n",
    "                tag = line[i].split('/')[1]\n",
    "                if tag=='nr' or tag=='ns' or tag=='nt':\n",
    "                    outp.write(word[0]+\"/B \")\n",
    "                    for j in word[1:len(word)]:\n",
    "                        if j!=' ':\n",
    "                            outp.write(j+\"/I \")\n",
    "                else:\n",
    "                    for wor in word:\n",
    "                        outp.write(wor+'/O ')\n",
    "                i+=1\n",
    "            outp.write('\\n')\n",
    "def sentence2split1():\n",
    "    with codecs.open('./validation3.txt','r','gbk') as inp,codecs.open('./validation4.txt','w','gbk') as outp:\n",
    "        texts = inp.read()\n",
    "        sentences = re.split('[，。！？、‘’“”:]/[O]', texts)\n",
    "        for sentence in sentences:\n",
    "\t        if sentence != \" \":\n",
    "\t\t        outp.write(sentence.strip()+'\\n')\n",
    "originHandle()\n",
    "originHandle2()\n",
    "sentence2split()\n",
    "originHandle3()\n",
    "originHandle4()\n",
    "sentence2split1()\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据读取成功\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import codecs\n",
    "\n",
    "datas = list()\n",
    "labels = list()\n",
    "\n",
    "datas1 = list()\n",
    "labels1 = list()\n",
    "\n",
    "#打开训练集，处理x，y\n",
    "with codecs.open('renmin4.txt','r','gbk') as train:\n",
    "    for line in train.readlines():\n",
    "        line = line.split()\n",
    "        linedata = []\n",
    "        linelabel = []\n",
    "        numNotO = 0\n",
    "        for word in line:\n",
    "            word = word.split('/')\n",
    "\n",
    "            linedata.append(word[0])\n",
    "            linelabel.append(word[1])\n",
    "            if word[1] != 'O':\n",
    "                numNotO += 1\n",
    "        if numNotO != 0:\n",
    "            datas.append(linedata)\n",
    "            labels.append(linelabel)\n",
    "#打开验证集，处理x，y\n",
    "with codecs.open('validation4.txt','r','gbk') as val:\n",
    "    for line in val.readlines():\n",
    "        line = line.split()\n",
    "        linedata1 = []\n",
    "        linelabel1 = []\n",
    "        numNotO1 = 0\n",
    "        for word in line:\n",
    "            word = word.split('/')\n",
    "            linedata1.append(word[0])\n",
    "            linelabel1.append(word[1])\n",
    "            if word[1] != 'O':\n",
    "                numNotO1 += 1\n",
    "        if numNotO1 != 0:\n",
    "            datas1.append(linedata1)\n",
    "            labels1.append(linelabel1)\n",
    "\n",
    "print('数据读取成功')\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Index!\n",
      "Finished!\n"
     ]
    }
   ],
   "source": [
    "#调用训练好的词向量\n",
    "import codecs\n",
    "import numpy as np\n",
    "from annoy import AnnoyIndex\n",
    "class PreTrainedEmbeddings(object):\n",
    "    \"\"\" A wrapper around pre-trained word vectors and their use \"\"\"\n",
    "    def __init__(self, word_to_index, word_vectors):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            word_to_index (dict): mapping from word to integers\n",
    "            word_vectors (list of numpy arrays)\n",
    "        \"\"\"\n",
    "        self.word_to_index = word_to_index\n",
    "        self.word_vectors = word_vectors\n",
    "        self.index_to_word = {v: k for k, v in self.word_to_index.items()}\n",
    "\n",
    "        self.index = AnnoyIndex(len(word_vectors[0]), metric='euclidean')\n",
    "        print(\"Building Index!\")\n",
    "        for _, i in self.word_to_index.items():\n",
    "            self.index.add_item(i, self.word_vectors[i])\n",
    "        self.index.build(50)\n",
    "        print(\"Finished!\")\n",
    "\n",
    "    @classmethod\n",
    "    def from_embeddings_file(cls, embedding_file):\n",
    "        \"\"\"Instantiate from pre-trained vector file.\n",
    "\n",
    "        Vector file should be of the format:\n",
    "            word0 x0_0 x0_1 x0_2 x0_3 ... x0_N\n",
    "            word1 x1_0 x1_1 x1_2 x1_3 ... x1_N\n",
    "\n",
    "        Args:\n",
    "            embedding_file (str): location of the file\n",
    "        Returns:\n",
    "            instance of PretrainedEmbeddigns\n",
    "        \"\"\"\n",
    "        word_to_index = {}\n",
    "        word_vectors = []\n",
    "\n",
    "        with codecs.open(embedding_file,encoding='utf-8') as fp:\n",
    "            for line in fp.readlines():\n",
    "                line = line.split(\" \")\n",
    "                word = line[0]\n",
    "                line = [x.strip() for x in line if x.strip()!='']\n",
    "                vec = np.array([float(x) for x in line[1:]])\n",
    "\n",
    "                word_to_index[word] = len(word_to_index)\n",
    "                word_vectors.append(vec)\n",
    "\n",
    "        return cls(word_to_index, word_vectors)\n",
    "\n",
    "    def get_embedding(self, word):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            word (str)\n",
    "        Returns\n",
    "            an embedding (numpy.ndarray)\n",
    "        \"\"\"\n",
    "        return self.word_vectors[self.word_to_index[word]]\n",
    "\n",
    "embeddings = PreTrainedEmbeddings.from_embeddings_file('ctb.50d.txt')\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "id2tag={0:'O',1:'B',2:'I'}\n",
    "def calculate(i,x,y,res=[]):\n",
    "    entity=[]\n",
    "    for j in range(len(x)):\n",
    "        if int(y[j][0])==0:\n",
    "            continue\n",
    "        if id2tag[int(y[j][0])]=='B':\n",
    "            entity=[datas1[i][j]]\n",
    "        elif id2tag[int(y[j][0])]=='I'  and len(entity)!=0 and j!=len(x)-1:\n",
    "            if id2tag[int(y[j+1][0])]!='O':\n",
    "                entity.append(datas1[i][j])\n",
    "            else:\n",
    "                entity.append(datas1[i][j])\n",
    "                res.append(entity)\n",
    "                entity=[]\n",
    "        elif id2tag[int(y[j][0])]=='I'  and len(entity)!=0 and j==len(x)-1:\n",
    "            entity.append(datas1[i][j])\n",
    "            res.append(entity)\n",
    "            entity=[]\n",
    "        else:\n",
    "            entity=[]\n",
    "    return res\n",
    "def calculate1(i,x,y,res=[]):\n",
    "    entity=[]\n",
    "    for j in range(len(x)):\n",
    "        if y[j]==0:\n",
    "            continue\n",
    "        if id2tag[y[j]]=='B':\n",
    "            entity=[datas1[i][j]]\n",
    "        elif id2tag[y[j]]=='I'  and len(entity)!=0 and j!=len(x)-1:\n",
    "            if id2tag[y[j+1]]!='O':\n",
    "                entity.append(datas1[i][j])\n",
    "            else:\n",
    "                entity.append(datas1[i][j])\n",
    "                res.append(entity)\n",
    "                entity=[]\n",
    "        elif id2tag[y[j]]=='I'  and len(entity)!=0 and j==len(x)-1:\n",
    "            entity.append(datas1[i][j])\n",
    "            res.append(entity)\n",
    "            entity=[]\n",
    "        else:\n",
    "            entity=[]\n",
    "    return res\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "#将训练集的x变成词向量\n",
    "x_train=[]\n",
    "for s1 in datas:\n",
    "    line=[]\n",
    "    for s2 in s1:\n",
    "        try:\n",
    "            emb=embeddings.get_embedding(s2)\n",
    "            line.append(emb)\n",
    "        except KeyError:\n",
    "            emb=embeddings.get_embedding('-unknown-')\n",
    "            line.append(emb)\n",
    "    x_train.append(line)\n",
    "\n",
    "\n",
    "y_train=labels\n",
    "for l1 in range(len(y_train)):\n",
    "    for l2 in range(len(y_train[l1])):\n",
    "        if y_train[l1][l2]=='O':\n",
    "            y_train[l1][l2]=0\n",
    "        elif y_train[l1][l2]=='B':\n",
    "            y_train[l1][l2]=1\n",
    "        else:\n",
    "            y_train[l1][l2]=2\n",
    "#将验证集的x变成词嵌入编码\n",
    "x_val=[]\n",
    "for s11 in datas1:\n",
    "    line1=[]\n",
    "    for s22 in s11:\n",
    "        try:\n",
    "            emb1=embeddings.get_embedding(s22)\n",
    "            line1.append(emb1)\n",
    "        except KeyError:\n",
    "            emb1=embeddings.get_embedding('-unknown-')\n",
    "            line1.append(emb1)\n",
    "    x_val.append(line1)\n",
    "y_val=labels1\n",
    "for l1 in range(len(y_val)):\n",
    "    for l2 in range(len(y_val[l1])):\n",
    "        if y_val[l1][l2]=='O':\n",
    "            y_val[l1][l2]=0\n",
    "        elif y_val[l1][l2]=='B':\n",
    "            y_val[l1][l2]=1\n",
    "        else:\n",
    "            y_val[l1][l2]=2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self,input_size,hidden_size,out_size):\n",
    "        super(BiLSTM, self).__init__()\n",
    "        self.hidden_size=hidden_size\n",
    "        self.out_size=out_size\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_size * 2, out_size)\n",
    "\n",
    "    def forward(self, X):\n",
    "        # X: [batch_size, max_len, n_class]\n",
    "        batch_size = X.shape[0]\n",
    "        input = X.transpose(0,1)\n",
    "        hidden_state = torch.randn(1*2, batch_size, self.hidden_size)   #\n",
    "        cell_state = torch.randn(1*2, batch_size, self.hidden_size)     #定义初始参数\n",
    "\n",
    "        outputs, (_, _) = self.lstm(input, (hidden_state, cell_state))\n",
    "        model = self.fc(outputs)\n",
    "        model=torch.squeeze(model)\n",
    "        return torch.softmax(model,dim=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-15-a2fafa2b58f2>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     16\u001B[0m         \u001B[0marr\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mFloatTensor\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0marr\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     17\u001B[0m         \u001B[0moptimizer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mzero_grad\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 18\u001B[1;33m         \u001B[0my_p\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmodel\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0marr\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     19\u001B[0m         \u001B[0mtrainy\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mLongTensor\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0my_train\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mi1\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     20\u001B[0m         \u001B[0mloss\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mcriterion\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0my_p\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mtrainy\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;31m# 计算损失\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mf:\\python\\knowlegde-project\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m    887\u001B[0m             \u001B[0mresult\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_slow_forward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    888\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 889\u001B[1;33m             \u001B[0mresult\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mforward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    890\u001B[0m         for hook in itertools.chain(\n\u001B[0;32m    891\u001B[0m                 \u001B[0m_global_forward_hooks\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<ipython-input-5-0ef80d3ee3f1>\u001B[0m in \u001B[0;36mforward\u001B[1;34m(self, X)\u001B[0m\n\u001B[0;32m     17\u001B[0m         \u001B[0mcell_state\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mrandn\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m*\u001B[0m\u001B[1;36m2\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mbatch_size\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mhidden_size\u001B[0m\u001B[1;33m)\u001B[0m     \u001B[1;31m# [num_layers(=1) * num_directions(=2), batch_size, n_hidden]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     18\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 19\u001B[1;33m         \u001B[0moutputs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0m_\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0m_\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlstm\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0mhidden_state\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcell_state\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     20\u001B[0m         \u001B[0mmodel\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfc\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0moutputs\u001B[0m\u001B[1;33m)\u001B[0m  \u001B[1;31m# model : [batch_size, n_class]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     21\u001B[0m         \u001B[0mmodel\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msqueeze\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mf:\\python\\knowlegde-project\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m    887\u001B[0m             \u001B[0mresult\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_slow_forward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    888\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 889\u001B[1;33m             \u001B[0mresult\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mforward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    890\u001B[0m         for hook in itertools.chain(\n\u001B[0;32m    891\u001B[0m                 \u001B[0m_global_forward_hooks\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mf:\\python\\knowlegde-project\\venv\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001B[0m in \u001B[0;36mforward\u001B[1;34m(self, input, hx)\u001B[0m\n\u001B[0;32m    659\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcheck_forward_args\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mhx\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mbatch_sizes\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    660\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mbatch_sizes\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 661\u001B[1;33m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001B[0m\u001B[0;32m    662\u001B[0m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001B[0;32m    663\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "def predict(x):\n",
    "        # Apply softmax to output\n",
    "        output = model(x)\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        return pred\n",
    "model=BiLSTM(50,50,3)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion=nn.CrossEntropyLoss()\n",
    "num_epoch = 30# 最大迭代更新次数\n",
    "for epoch in range(num_epoch):\n",
    "    i1=0\n",
    "    losses=0\n",
    "    for ar in x_train:\n",
    "        arr=[]\n",
    "        arr.append(ar)\n",
    "        arr=torch.FloatTensor(arr)\n",
    "        optimizer.zero_grad()\n",
    "        y_p = model(arr)\n",
    "        trainy=torch.LongTensor(y_train[i1])\n",
    "        loss = criterion(y_p,trainy)# 计算损失\n",
    "        losses+=loss\n",
    "        loss.backward()  # 计算梯度，误差回传\n",
    "        optimizer.step()#根据计算的梯度，更新网络中的参数\n",
    "        i1+=1\n",
    "    if epoch % 1 == 0:\n",
    "        print('epoch: {}, loss: {}'.format(epoch, losses/i1))\n",
    "        entityres=[]\n",
    "        entityall=[]\n",
    "        i2=0\n",
    "        f1=0\n",
    "        for ar1 in x_val:\n",
    "            arr1=[]\n",
    "            arr1.append(ar1)\n",
    "            arr1=torch.FloatTensor(arr1)\n",
    "            y_p1 = predict(arr1)\n",
    "            entityres = calculate(i2,ar1,y_p1,entityres)\n",
    "            entityall = calculate1(i2,ar1,y_val[i2],entityall)\n",
    "            i2+=1\n",
    "        jiaoji = [i for i in entityres if i in entityall]\n",
    "        if len(jiaoji)!=0:\n",
    "            zhun = float(len(jiaoji))/len(entityres)\n",
    "            quan = float(len(jiaoji))/len(entityall)\n",
    "            f1=(2*zhun*quan)/(zhun+quan)\n",
    "            print (\"val:\")\n",
    "            print (\"查准率:\", zhun)\n",
    "            print (\"查全率:\", quan)\n",
    "            print (\"f1:\", f1)\n",
    "            print(\"\\n\")\n",
    "        else:\n",
    "            print (\"查准率:\",0)\n",
    "            print(\"\\n\")\n",
    "        if f1>0.76:\n",
    "            break\n",
    "print(\"Done\")\n",
    "torch.save(model.state_dict(),\"work23.pt\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2],\n",
      "        [2]])\n"
     ]
    }
   ],
   "source": [
    "def predict(x):\n",
    "        # Apply softmax to output\n",
    "        output = model1(x)\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        return pred\n",
    "model1=BiLSTM(50,50,3)\n",
    "model1.load_state_dict(torch.load(\"work23.pt\"))\n",
    "s=\"北 京 理 工 大 学\"\n",
    "s=s.split()\n",
    "s1=[]\n",
    "i=0\n",
    "for l in s:\n",
    "    s[i]=embeddings.get_embedding(l)\n",
    "    i+=1\n",
    "s1.append(s)\n",
    "s1=torch.FloatTensor(s1)\n",
    "ss=predict(s1)\n",
    "print(ss)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}